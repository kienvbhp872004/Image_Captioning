{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-22T15:16:03.843901Z",
     "start_time": "2025-04-22T15:16:00.479694Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import *\n",
    "from dataset import FlickrDataset\n",
    "\n",
    "img_root = r'D:\\git\\Image_Captioning\\dataset\\Images'\n",
    "caption_root = r'D:\\git\\Image_Captioning\\dataset\\captions.txt'\n",
    "def train():\n",
    "\n",
    "    vocab_size = 25\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    def collate_fn(batch):\n",
    "        images = []\n",
    "        captions = []\n",
    "        for img, caption in batch:\n",
    "            images.append(img)\n",
    "            while len(caption) < vocab_size:\n",
    "                caption.append(0)\n",
    "            captions.append(caption)\n",
    "        return images, captions\n",
    "    dataset = FlickrDataset(img_root, caption_root, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "train()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7822, -0.2342, -0.0629,  ..., -2.0494, -2.0152, -2.0323],\n",
      "         [-0.8335, -0.1999, -0.0458,  ..., -1.9980, -1.9980, -1.9467],\n",
      "         [-0.8678, -0.1657, -0.0458,  ..., -1.9809, -2.0152, -1.8782],\n",
      "         ...,\n",
      "         [ 0.9646,  0.2624,  0.7077,  ...,  1.1015,  0.7419,  0.7077],\n",
      "         [ 1.0331,  1.1872,  1.6838,  ...,  1.0502,  0.7762,  0.7591],\n",
      "         [ 1.3927,  1.5468,  0.8276,  ...,  0.9817,  0.7762,  0.7762]],\n",
      "\n",
      "        [[-0.5826,  0.1877,  0.3627,  ..., -1.9832, -1.9132, -1.8957],\n",
      "         [-0.6527,  0.2052,  0.3978,  ..., -1.9132, -1.8606, -1.7381],\n",
      "         [-0.7227,  0.2402,  0.4153,  ..., -1.8782, -1.7906, -1.5455],\n",
      "         ...,\n",
      "         [-0.2675, -0.8277, -0.0574,  ...,  1.4832,  1.1856,  1.1856],\n",
      "         [-0.2850,  0.5728,  0.9930,  ...,  1.4132,  1.1856,  1.1856],\n",
      "         [ 0.0651,  0.5903, -0.2150,  ...,  1.3606,  1.2031,  1.1856]],\n",
      "\n",
      "        [[-0.1835,  0.3219,  0.5659,  ..., -1.7522, -1.7173, -1.7347],\n",
      "         [-0.2532,  0.3568,  0.6531,  ..., -1.6999, -1.6999, -1.6999],\n",
      "         [-0.2707,  0.4614,  0.6879,  ..., -1.6999, -1.7173, -1.6824],\n",
      "         ...,\n",
      "         [-1.3687, -1.5430, -0.7413,  ...,  2.1694,  1.8208,  1.7163],\n",
      "         [-1.3861, -0.8284, -0.5670,  ...,  2.0997,  1.7685,  1.6465],\n",
      "         [-1.0550, -0.6715, -0.8110,  ...,  2.0997,  1.7337,  1.6117]]]), tensor([ 1,  4,  5,  6,  4,  7,  8,  9, 10, 11,  4, 12, 13, 14,  6, 15,  3, 16,\n",
      "        17,  2]))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e7e3f2f49d3d1cd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
