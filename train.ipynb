{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T13:59:56.929612Z",
     "start_time": "2025-04-23T13:59:17.938410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import FlickrDataset\n",
    "from model import *\n",
    "import tqdm\n",
    "img_root = r'D:\\git\\Image_Captioning\\dataset\\Images'\n",
    "caption_root = r'D:\\git\\Image_Captioning\\dataset\\captions.txt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vocab_size = 25\n",
    "hidden_size = 512\n",
    "embedding_dim = 256\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    captions = []\n",
    "    max_len = max(len(caption) for _, caption in batch)  # Find max caption length in batch\n",
    "    for img, caption in batch:\n",
    "        images.append(img)\n",
    "        # Pad caption to max_len\n",
    "        pad_tensor = torch.zeros(max_len - len(caption)).long()  # Use <PAD> token index\n",
    "        padded_caption = torch.cat((caption, pad_tensor), dim=0)\n",
    "        captions.append(padded_caption)\n",
    "\n",
    "    # Stack images and captions into tensors\n",
    "    images = torch.stack(images, dim=0)  # Shape: (batch_size, C, H, W)\n",
    "    captions = torch.stack(captions, dim=0)  # Shape: (batch_size, max_len)\n",
    "    return images, captions\n",
    "dataset = FlickrDataset(img_root, caption_root, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ],
   "id": "e7e3f2f49d3d1cd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:00:13.102900Z",
     "start_time": "2025-04-23T13:59:57.150933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = CNNtoRNN(vocab_size, hidden_size, embedding_dim, num_layers=1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])  # Rename for clarity\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img, captions in tqdm.tqdm(dataloader):\n",
    "        img = img.to(device)  # Shape: (batch_size, C, H, W)\n",
    "        captions = captions.to(device)  # Shape: (batch_size, max_len)\n",
    "        # Compute lengths (exclude <EOS> or <PAD>)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img, captions)  # Ensure model accepts lengths if needed\n",
    "\n",
    "        # Prepare targets\n",
    "        targets = captions[:, 1:]  # Exclude <SOS>\n",
    "        packed_targets = pack_padded_sequence(targets, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "        packed_outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False)[0]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(packed_outputs, packed_targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "ee21154ceaf6294f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\miniconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/1265 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "start (24) + length (1) exceeds dimension size (24).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptions\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Ensure model accepts lengths if needed\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Prepare targets\u001B[39;00m\n\u001B[0;32m     21\u001B[0m targets \u001B[38;5;241m=\u001B[39m captions[:, \u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# Exclude <SOS>\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\git\\Image_Captioning\\model.py:72\u001B[0m, in \u001B[0;36mCNNtoRNN.forward\u001B[1;34m(self, images, captions)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, captions):\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# images: (batch_size, 3, 299, 299)\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# captions: (batch_size, caption_length)\u001B[39;00m\n\u001B[0;32m     71\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoderCNN(images)  \u001B[38;5;66;03m# (batch_size, embed_size)\u001B[39;00m\n\u001B[1;32m---> 72\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoderRNN\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlengths\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (batch_size, caption_length + 1, vocab_size)\u001B[39;00m\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\git\\Image_Captioning\\model.py:43\u001B[0m, in \u001B[0;36mDecoderRNN.forward\u001B[1;34m(self, features, captions, lengths)\u001B[0m\n\u001B[0;32m     41\u001B[0m inputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((features, embeddings), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (batch_size, caption_length + 1, embed_size)\u001B[39;00m\n\u001B[0;32m     42\u001B[0m packed \u001B[38;5;241m=\u001B[39m pack_padded_sequence(inputs, lengths, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, enforce_sorted\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 43\u001B[0m hidden, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpacked\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (batch_size, caption_length + 1, hidden_size)\u001B[39;00m\n\u001B[0;32m     44\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(hidden[\u001B[38;5;241m0\u001B[39m])  \u001B[38;5;66;03m# (batch_size, caption_length + 1, vocab_size)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1136\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1124\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1125\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1126\u001B[0m         hx,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1133\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first,\n\u001B[0;32m   1134\u001B[0m     )\n\u001B[0;32m   1135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1136\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1146\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1147\u001B[0m output \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1148\u001B[0m hidden \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m1\u001B[39m:]\n",
      "\u001B[1;31mRuntimeError\u001B[0m: start (24) + length (1) exceeds dimension size (24)."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T17:57:13.533059Z",
     "start_time": "2025-04-25T17:19:31.777135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "from dataset import FlickrDataset  # Giả sử bạn đã định nghĩa lớp này\n",
    "from vocab import Vocabulary  # Giả sử bạn đã định nghĩa lớp này\n",
    "\n",
    "# Định nghĩa các lớp mô hình\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet101(weights=\"IMAGENET1K_V1\")\n",
    "        modules = list(resnet.children())[:-1]  # Loại bỏ lớp FC\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False  # Đóng băng ResNet\n",
    "        self.linear = nn.Linear(2048, embed_size)  # ResNet-101 feature size\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)  # (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (batch_size, 2048)\n",
    "        features = self.bn(self.linear(features))  # (batch_size, embed_size)\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # features: (batch_size, embed_size)\n",
    "        # captions: (batch_size, max_len) chứa các token từ <SOS> đến <EOS> hoặc được pad\n",
    "        embeddings = self.dropout(self.embed(captions[:, :-1]))  # Loại bỏ <EOS>, (batch_size, max_len-1, embed_size)\n",
    "        features = features.unsqueeze(1)  # (batch_size, 1, embed_size)\n",
    "        inputs = torch.cat((features, embeddings), dim=1)  # (batch_size, max_len, embed_size)\n",
    "        outputs, _ = self.lstm(inputs)  # (batch_size, max_len, hidden_size)\n",
    "        outputs = self.linear(outputs)  # (batch_size, max_len, vocab_size)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, max_len=25):\n",
    "        sample_ids = []\n",
    "        inputs = features.unsqueeze(1)  # (batch_size, 1, embed_size)\n",
    "        states = None\n",
    "        for _ in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)  # (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))  # (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)  # (batch_size,)\n",
    "            sample_ids.append(predicted)\n",
    "            inputs = self.embed(predicted).unsqueeze(1)  # (batch_size, 1, embed_size)\n",
    "        return torch.stack(sample_ids, dim=1)  # (batch_size, max_len)\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)  # (batch_size, embed_size)\n",
    "        outputs = self.decoderRNN(features, captions)  # (batch_size, max_len, vocab_size)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_len=25):\n",
    "        result_caption = []\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)  # (1, embed_size)\n",
    "            states = None\n",
    "            for _ in range(max_len):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)  # (1, 1, hidden_size)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(1))  # (1, vocab_size)\n",
    "                predicted = output.argmax(1)  # (1,)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(1)  # (1, 1, embed_size)\n",
    "                if vocabulary.itos[predicted.item()] == \"<END>\":\n",
    "                    break\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]\n",
    "\n",
    "# Hàm collate_fn để xử lý batch\n",
    "def collate_fn(batch, max_len=25):\n",
    "    images = []\n",
    "    captions = []\n",
    "    for img, caption in batch:\n",
    "        images.append(img)\n",
    "        # Cắt hoặc pad caption để đạt độ dài max_len\n",
    "        if len(caption) > max_len:\n",
    "            caption = caption[:max_len]\n",
    "        else:\n",
    "            pad_tensor = torch.ones(max_len - len(caption)).long() * 0  # <PAD> token\n",
    "            caption = torch.cat((caption, pad_tensor), dim=0)\n",
    "        captions.append(caption)\n",
    "    images = torch.stack(images, dim=0)  # (batch_size, C, H, W)\n",
    "    captions = torch.stack(captions, dim=0)  # (batch_size, max_len)\n",
    "    return images, captions\n",
    "\n",
    "# Cấu hình\n",
    "img_root = r'D:\\git\\Image_Captioning\\dataset\\Images'\n",
    "caption_root = r'D:\\git\\Image_Captioning\\dataset\\captions.txt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_size = 512\n",
    "embedding_dim = 256\n",
    "num_layers = 1\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Transform cho dữ liệu\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Dataset và DataLoader\n",
    "dataset = FlickrDataset(img_root, caption_root, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "vocab_size = len(dataset.vocab)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "# Khởi tạo mô hình, loss và optimizer\n",
    "model = CNNtoRNN(embedding_dim, hidden_size, vocab_size, num_layers).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Bỏ qua <PAD>\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Huấn luyện\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (images, captions) in tqdm.tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        images = images.to(device)  # (batch_size, C, H, W)\n",
    "        captions = captions.to(device)  # (batch_size, max_len)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, captions)  # (batch_size, max_len, vocab_size)\n",
    "        targets = captions[:, 1:]  # Loại bỏ <SOS>, (batch_size, max_len-1)\n",
    "        outputs = outputs[:, :-1, :]  # Loại bỏ bước cuối, (batch_size, max_len-1, vocab_size)\n",
    "\n",
    "        # Reshape cho CrossEntropyLoss\n",
    "        outputs = outputs.reshape(-1, vocab_size)  # (batch_size * (max_len-1), vocab_size)\n",
    "        targets = targets.reshape(-1)  # (batch_size * (max_len-1))\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch: {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Lưu mô hình sau mỗi epoch\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
   ],
   "id": "c2d1a52ac950abee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [11:06<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 4.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [10:56<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Average Loss: 3.4985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [11:50<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Average Loss: 3.2759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 369/1265 [03:26<08:21,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 151\u001B[0m\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;66;03m# Backward\u001B[39;00m\n\u001B[0;32m    150\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m--> 151\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclip_grad_norm_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    154\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(dataloader)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001B[0m, in \u001B[0;36m_no_grad.<locals>._no_grad_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_no_grad_wrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 34\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:215\u001B[0m, in \u001B[0;36mclip_grad_norm_\u001B[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001B[0m\n\u001B[0;32m    213\u001B[0m     parameters \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(parameters)\n\u001B[0;32m    214\u001B[0m grads \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m parameters \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m--> 215\u001B[0m total_norm \u001B[38;5;241m=\u001B[39m \u001B[43m_get_total_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_if_nonfinite\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_norm\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001B[0m, in \u001B[0;36m_no_grad.<locals>._no_grad_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_no_grad_wrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 34\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:98\u001B[0m, in \u001B[0;36m_get_total_norm\u001B[1;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     93\u001B[0m         norms\u001B[38;5;241m.\u001B[39mextend(\n\u001B[0;32m     94\u001B[0m             [torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mvector_norm(g, norm_type) \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m device_tensors]\n\u001B[0;32m     95\u001B[0m         )\n\u001B[0;32m     97\u001B[0m total_norm \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mvector_norm(\n\u001B[1;32m---> 98\u001B[0m     torch\u001B[38;5;241m.\u001B[39mstack([\u001B[43mnorm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfirst_device\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m norm \u001B[38;5;129;01min\u001B[39;00m norms]), norm_type\n\u001B[0;32m     99\u001B[0m )\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error_if_nonfinite \u001B[38;5;129;01mand\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mlogical_or(total_norm\u001B[38;5;241m.\u001B[39misnan(), total_norm\u001B[38;5;241m.\u001B[39misinf()):\n\u001B[0;32m    102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    103\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe total norm of order \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnorm_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for gradients from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    104\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    105\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    106\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `error_if_nonfinite=False`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    107\u001B[0m     )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T17:19:18.994559Z",
     "start_time": "2025-04-25T17:19:11.066546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU detected'}\")"
   ],
   "id": "ad7e84663de37b3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU device: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
